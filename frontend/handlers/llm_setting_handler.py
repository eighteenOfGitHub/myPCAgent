# frontend/handlers/llm_setting_handler.py

import requests
from typing import Optional, List, Tuple
from config.env_config import env_config
from shared.llm_setting_schemas import (
    LLMConfigCreate,
    LLMProvider,
    LLMTestResponse,
    LLMConfigResponse,
    LLMConfigBasicResponse,
)
from shared.user_preference_schemas import UserPreferenceResponse, SetDefaultLLMResponse

# é‡è¦è¿‡ç¨‹ï¼šä» env_config ç»Ÿä¸€è·å– API åŸºå€
API_BASE = env_config.API_BASE_URL

def submit_new_llm_config(
    provider: LLMProvider,
    model_name: str,
    api_key: str | None,
    base_url: Optional[str] = None
) -> tuple[bool, str]:
    """Handler for 'Submit' button to save new LLM configuration."""
    if provider == LLMProvider.OLLAMA and api_key is None:
        api_key = ""
    
    # æäº¤å‰åŠ å¯† api_key
    from shared.crypto import encrypt_text
    encrypted_api_key = encrypt_text(api_key) if api_key else ""
        
    config_data = LLMConfigCreate(
        provider=provider,
        model_name=model_name,
        api_key=encrypted_api_key,
        base_url=base_url
    )

    try:
        response = requests.post(
            url=f"{API_BASE}/settings/llm",
            json=config_data.model_dump(),
            timeout=30
        )

        if response.status_code == 200:
            saved_config = LLMConfigResponse.model_validate(response.json())
            return True, f"æ¨¡å‹ '{saved_config.model_name}' é…ç½®å·²æˆåŠŸä¿å­˜ï¼"
        else:
            error_detail = _get_error_detail(response)
            return False, f"è¯·æ±‚å¤±è´¥: {error_detail}"

    except requests.exceptions.Timeout:
        return False, "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡å™¨çŠ¶æ€ã€‚"
    except requests.exceptions.ConnectionError:
        return False, "æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡å™¨ï¼Œè¯·ç¡®è®¤åç«¯æœåŠ¡å·²å¯åŠ¨ã€‚"
    except Exception as e:
        return False, f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

def test_existing_llm_config(config_id: int) -> str:
    """Handler for 'ğŸŸ¢ Test' button on an existing config row."""
    try:
        response = requests.post(
            url=f"{API_BASE}/settings/llm/{config_id}/test",
            timeout=30
        )
        if response.status_code == 200:
            result = LLMTestResponse.model_validate(response.json())
            return result.message or ("æµ‹è¯•é€šè¿‡" if result.success else "æµ‹è¯•å¤±è´¥")
        else:
            error_detail = _get_error_detail(response)
            return f"æµ‹è¯•å¤±è´¥: {error_detail}"
    except Exception as e:
        return f"æµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

def delete_llm_config(config_id: int) -> bool:
    """Handler for 'ğŸ—‘ï¸ Delete' button on an existing config row."""
    try:
        response = requests.delete(
            url=f"{API_BASE}/settings/llm/{config_id}",
            timeout=10
        )
        if response.status_code == 200:
            from shared.general_schemas import MessageResponse
            MessageResponse.model_validate(response.json())
            return True
        return False
    except Exception:
        return False

def get_all_llm_configs() -> tuple[bool, list | str]:
    """Handler function to fetch all LLM configurations from the backend API."""
    try:
        response = requests.get(
            url=f"{API_BASE}/settings/llm/",
            timeout=30
        )

        if response.status_code == 200:
            configs_list = [LLMConfigResponse.model_validate(cfg) for cfg in response.json()]
            return True, [cfg.model_dump() for cfg in configs_list]
        else:
            error_detail = _get_error_detail(response)
            return False, f"è·å–é…ç½®åˆ—è¡¨å¤±è´¥: {error_detail}"

    except requests.exceptions.Timeout:
        return False, "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æœåŠ¡å™¨çŠ¶æ€ã€‚"
    except requests.exceptions.ConnectionError:
        return False, "æ— æ³•è¿æ¥åˆ°åç«¯æœåŠ¡å™¨ï¼Œè¯·ç¡®è®¤åç«¯æœåŠ¡å·²å¯åŠ¨ã€‚"
    except requests.exceptions.RequestException as e:
        return False, f"è¯·æ±‚å‘ç”Ÿé”™è¯¯: {str(e)}"
    except Exception as e:
        return False, f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

def fetch_llm_basic_options() -> List[Tuple[str, int]]:
    """è·å–ä¸‹æ‹‰å¯é€‰é¡¹ï¼Œè¿”å› [(label, id), ...]"""
    try:
        resp = requests.get(f"{API_BASE}/settings/llm/basic", timeout=10)
        resp.raise_for_status()
        items = resp.json()
        validated = [LLMConfigBasicResponse.model_validate(item) for item in items]
        return [
            (f"{item.provider.value} / {item.model_name}", item.id)
            for item in validated
        ]
    except Exception:
        return []

def fetch_default_llm_config_id() -> Optional[int]:
    """è·å–åç«¯ä¿å­˜çš„é»˜è®¤ LLM é…ç½® IDï¼Œå¤±è´¥è¿”å› None"""
    try:
        resp = requests.get(f"{API_BASE}/preference", timeout=10)
        resp.raise_for_status()
        pref = UserPreferenceResponse.model_validate(resp.json())
        return pref.default_llm_config_id
    except Exception:
        return None

def set_default_llm_config(config_id: int | None) -> tuple[bool, str, int | None]:
    """ä¿å­˜é»˜è®¤ LLM é…ç½®ï¼Œconfig_id ä¸º None è¡¨ç¤ºæ¸…ç©ºã€‚"""
    try:
        params = {"config_id": config_id} if config_id is not None else {}
        resp = requests.post(f"{API_BASE}/preference/default-llm", params=params, timeout=10)
        resp.raise_for_status()
        result = SetDefaultLLMResponse.model_validate(resp.json())
        return True, "Saved default model successfully.", result.default_llm_config_id
    except Exception as e:
        return False, f"Save default model failed: {e}", None

def _get_error_detail(response):
    try:
        return response.json().get("detail", f"HTTP Error: {response.status_code}")
    except ValueError:
        return f"HTTP Error: {response.status_code}, Response Text: {response.text}"